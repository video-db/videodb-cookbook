{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/video-db/videodb-cookbook/blob/nb/censor/examples/Scene_index_toolkit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install videodb openai Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"VIDEO_DB_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Your Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import connect \n",
    "conn = connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ðŸš¨ ATTENTION: avoid re-upload the video, use already uploaded video by using the video_id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = conn.get_collection()\n",
    "\n",
    "# find Video by query\n",
    "video = None\n",
    "query = \"Arya\"\n",
    "for v in coll.get_videos():\n",
    "    if query in v.name:\n",
    "        video = v\n",
    "\n",
    "#find by id\n",
    "# video = coll.get_video(\"SOMEID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = conn.get_collection()\n",
    "\n",
    "video = conn.upload(url=\"https://www.youtube.com/watch?v=6xOiVAW54CQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video.index_scenes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 6.439766666666666\n",
      "The image features a close-up of a man with a rugged appearance, set against a dark, indistinct background. His expression is complex, with furrowed brows and eyes that gaze directly ahead, conveying intensity or concern. The lighting accentuates his features, highlighting creases on his forehead and subtle stubble across his face. Shadows envelop the periphery of the image, focusing attention on the central figure's visage. The color palette is warm yet muted, dominated by browns and ambers that suggest an intimate or somber setting. There is an air of raw emotion, suggesting the subject is in a moment of contemplation or inner turmoil.\n"
     ]
    }
   ],
   "source": [
    "scenes = video.get_scenes()\n",
    "print(f\"{scenes[0]['start']} - {scenes[0]['end']}\")\n",
    "print(scenes[0][\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene Detected from video :  56\n"
     ]
    }
   ],
   "source": [
    "print(\"Scene Detected from video : \", len(scenes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Scene Index Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `overlay_text_image_width` : Width of Overlay Image, that display scene description\n",
    "- `overlay_text_image_heiht` : Height of Overlay Image, that display scene description\n",
    "- `overlay_text_iamge_size` : Font Size of Text in Overlay Image\n",
    "- `images_dir` : The directory where images would be generated and saved\n",
    "\n",
    "\n",
    "#### **ðŸš¨ ATTENTION: avoid re-upload the video, use already uploaded video by using the video_id**\n",
    "the string in `get_image_name` function is going to decide name for your overlay images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_text_image_width = 800\n",
    "overlay_text_image_height = 600\n",
    "overlay_text_image_size = 20\n",
    "text_color = (0, 0, 0, 255)\n",
    "images_dir = \"images\"\n",
    "\n",
    "\n",
    "def get_image_path(index):\n",
    "    return f\"{images_dir}/{get_image_name(index)}.png\"\n",
    "\n",
    "def get_image_name(index):\n",
    "    return f\"myimage-2-{index}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import math\n",
    "\n",
    "\n",
    "def create_image_with_text(text, file_path):\n",
    "    padding = 50\n",
    "\n",
    "    width, height = overlay_text_image_width, overlay_text_image_height\n",
    "    image = Image.new(\"RGBA\", (width, height), (255, 255, 255, 0))\n",
    "    font = ImageFont.load_default(size=overlay_text_image_size)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    text_color = (0, 0, 0, 255)\n",
    "\n",
    "    text_length = draw.textlength(text, font=font)\n",
    "    avg_len_per_char = text_length / len(text)\n",
    "    character_per_line = math.floor((width - 2 * padding) / avg_len_per_char)\n",
    "    newline_text = \"\\n\".join(\n",
    "        text[i : i + character_per_line]\n",
    "        for i in range(0, len(text), character_per_line)\n",
    "    )\n",
    "\n",
    "    # Calculate text size and position for centering, using the correct method\n",
    "    draw.multiline_text(text=newline_text, font=font, xy=[0, 0], fill=text_color)\n",
    "\n",
    "    # Save the image\n",
    "    image.save(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When conducting tests, it's advisable to start with a limited number of scenarios to avoid overwhelming the database unnecessarily. \n",
    "\n",
    "Begin your testing with a few select scenarios, and only proceed to test with all scenarios once you're confident in your approach. \n",
    "\n",
    "This strategy helps manage resource use effectively during the testing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proccess only first 5 scene\n",
    "# comment out this line, if you want to process all scenes\n",
    "scenes = scenes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded Image - {} 0\n",
      "Uploaded Image - {} 1\n",
      "Uploaded Image - {} 2\n",
      "Uploaded Image - {} 3\n",
      "Uploaded Image - {} 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://console.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/c404b58a-a306-427f-850b-a24a410868c2.m3u8'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from videodb.timeline import Timeline\n",
    "from videodb.asset import VideoAsset, TextAsset, ImageAsset\n",
    "from videodb import play_stream, TextStyle, MediaType\n",
    "\n",
    "timeline = Timeline(conn)\n",
    "\n",
    "video_asset = VideoAsset(asset_id=video.id)\n",
    "timeline.add_inline(video_asset)\n",
    "\n",
    "if not os.path.exists(images_dir):\n",
    "    os.makedirs(images_dir)\n",
    "\n",
    "for index, scene in enumerate(scenes):\n",
    "    scene_start = float(scene[\"start\"])\n",
    "    scene_end = float(scene[\"end\"])\n",
    "    scene_desc = scene[\"response\"]\n",
    "    duration = scene_end - scene_start\n",
    "    image_path = get_image_path(index)\n",
    "\n",
    "    print(\"Uploaded Image - {}\", index)\n",
    "\n",
    "    # Create an image with the scene description Text \n",
    "    create_image_with_text(scene_desc, image_path)\n",
    "\n",
    "    # Upload the image to VideoDB\n",
    "    image = conn.upload(file_path=image_path, media_type=MediaType.image)\n",
    "\n",
    "    # Ovelay the image on the video\n",
    "    timeline.add_overlay(\n",
    "        start=scene_start,\n",
    "        asset=ImageAsset(\n",
    "            asset_id=image.id,\n",
    "            duration=duration,\n",
    "            width=overlay_text_image_width,\n",
    "            height=overlay_text_image_height,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "stream_url = timeline.generate_stream()\n",
    "play_stream(stream_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to generate the timeline again, its better to use the same images instead of uploading them to VideoDB again,\n",
    "you can use this cell to do the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://console.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/d947d087-bd49-4980-8eb8-8fff15f51e1b.m3u8'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from videodb.timeline import Timeline\n",
    "from videodb.asset import VideoAsset, TextAsset, ImageAsset\n",
    "from videodb import play_stream, TextStyle, MediaType\n",
    "\n",
    "all_images = coll.get_images()\n",
    "\n",
    "timeline = Timeline(conn)\n",
    "\n",
    "video_asset = VideoAsset(asset_id=video.id)\n",
    "timeline.add_inline(video_asset)\n",
    "\n",
    "if not os.path.exists(images_dir):\n",
    "    os.makedirs(images_dir)\n",
    "\n",
    "for index, scene in enumerate(scenes):\n",
    "    scene_start = float(scene[\"start\"])\n",
    "    scene_end = float(scene[\"end\"])\n",
    "    scene_desc = scene[\"response\"]\n",
    "    duration = scene_end - scene_start\n",
    "    image_name = get_image_name(index)\n",
    "\n",
    "    target_image = None\n",
    "\n",
    "    for image in all_images:\n",
    "        if image_name in image.name:\n",
    "            target_image = image\n",
    "            break\n",
    "    \n",
    "    if(target_image):\n",
    "        # Ovelay the image on the video\n",
    "        timeline.add_overlay(\n",
    "            start=scene_start,\n",
    "            asset=ImageAsset(\n",
    "                asset_id=target_image.id,\n",
    "                duration=duration,\n",
    "                width=overlay_text_image_width,\n",
    "                height=overlay_text_image_height,\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        print(\"Image not found in VideoDB - {}\", index)\n",
    "# \n",
    "stream_url = timeline.generate_stream()\n",
    "play_stream(stream_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing LLM Decider Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_prompt = \"\"\"Hereâ€™s the data from a vision (scene) index run on a video footage. The structure of the data is { - index: scene description}. \n",
    "Based on this information, identify and filter the scenes safe for mature audiences but do not include nudity.\n",
    "As a response, return a json which contains one fields, safe and its value is a list of all indexes(numbers) that are safe \\n \\n\"\"\"\n",
    "\n",
    "for index, scene in enumerate(scenes):\n",
    "    llm_prompt += f\"{index}: {scene['response']}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "openai_res = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"system\", \"content\": llm_prompt}],\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "openai_res = json.loads(openai_res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.439766666666666, 7.841166666666667], [7.841166666666667, 9.542866666666667], [22.989633333333334, 27.2272]]\n",
      "[[0.0, 6.439766666666666], [9.542866666666667, 22.989633333333334]]\n"
     ]
    }
   ],
   "source": [
    "safe_indexes = [int(item) for item in openai_res['safe']]\n",
    "unsafe_indexes = [item for item in range(len(scenes)) if item not in openai_res['safe']]\n",
    "\n",
    "safe_shots = []\n",
    "unsafe_shots = []\n",
    "\n",
    "for index in safe_indexes: \n",
    "    safe_shots.append([float(scenes[index]['start']), float(scenes[index]['end'])])\n",
    "\n",
    "for index in unsafe_indexes: \n",
    "    unsafe_shots.append([float(scenes[index]['start']), float(scenes[index]['end'])])\n",
    "\n",
    "\n",
    "print(safe_shots)\n",
    "print(unsafe_shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://console.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/2a16e42b-e34d-469a-a752-9983bf8a3529.m3u8'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, scene in enumerate(scenes):\n",
    "    scene_start = float(scene[\"start\"])\n",
    "    scene_end = float(scene[\"end\"])\n",
    "    duration = scene_end - scene_start\n",
    "\n",
    "    if(index in safe_indexes):\n",
    "        text = \"safe\"\n",
    "        color = 'green'\n",
    "    else:\n",
    "        text = \"unsafe\"\n",
    "        color = 'red'\n",
    "    text_asset = TextAsset(text=text, style=TextStyle(fontcolor=color, fontsize=30), duration=duration)\n",
    "    timeline.add_overlay(start=scene_start, asset=text_asset)\n",
    "\n",
    "\n",
    "stream_url = timeline.generate_stream()\n",
    "play_stream(stream_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Safe Stream and Unsafe Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_timestamps(shots, threshold):\n",
    "    merged_shots = [shots[0]]  \n",
    "\n",
    "    for current_start, current_end in shots[1:]:\n",
    "        last_end = merged_shots[-1][1]\n",
    "\n",
    "        # If the current start time is within the threshold of the last end time, merge them.\n",
    "        if current_start - last_end <= threshold:\n",
    "            merged_shots[-1][1] = max(last_end, current_end)  # Update the end time of the last interval.\n",
    "        else:\n",
    "            # Otherwise, add the current interval as a new entry.\n",
    "            merged_shots.append([current_start, current_end])\n",
    "\n",
    "    return merged_shots \n",
    "\n",
    "\n",
    "merged_safe_shots = merge_timestamps(safe_shots, 1)\n",
    "merged_unsafe_shots = merge_timestamps(unsafe_shots, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streams of Shot Detected as Safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://console.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/eea2dbfe-dc30-4adc-8db7-537ac627a51c.m3u8'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from videodb import play_stream\n",
    "\n",
    "stream_url = video.generate_stream(timeline=merged_safe_shots)\n",
    "play_stream(stream_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streams of Shot Detected as UnSafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://console.videodb.io/player?url=https://stream.videodb.io/v3/published/manifests/fe772d36-9d3a-4639-9100-2291341bb8b5.m3u8'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from videodb import play_stream\n",
    "\n",
    "stream_url = video.generate_stream(timeline=merged_unsafe_shots)\n",
    "play_stream(stream_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete Uploaded Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb.timeline import Timeline\n",
    "from videodb.asset import VideoAsset, TextAsset, ImageAsset\n",
    "from videodb import play_stream, TextStyle, MediaType\n",
    "\n",
    "all_images = coll.get_images()\n",
    "\n",
    "for index, scene in enumerate(scenes):\n",
    "    image_name = get_image_name(index)\n",
    "    target_image = None\n",
    "    for image in all_images:\n",
    "        if image_name in image.name:\n",
    "            target_image = image\n",
    "            break\n",
    "    if(target_image):\n",
    "        print(\"Image Deleted - {}\", index)\n",
    "        image.delete()\n",
    "    else:\n",
    "        print(\"Image not found in VideoDB - {}\", index)\n",
    "# \n",
    "stream_url = timeline.generate_stream()\n",
    "play_stream(stream_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
